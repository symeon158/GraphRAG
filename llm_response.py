# llm_response.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
import os

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("Missing OpenAI API Key. Set it in the .env file.")


llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=openai_api_key)

def generate_response(user_query, graph_data, mode="text_only"):
    """
    Generates a structured Greek response based on knowledge graph data.
    mode: "text_only" or "hybrid"
    """

    if mode == "text_only":
        # Old structure: node_1, relationship, node_2
        context = "\n".join([
            f"â¤ **{item['node_1']}** â†’ {item['relationship']} â†’ **{item['node_2']}**"
            for item in graph_data
        ])
    else:
        # Hybrid structure includes a 'score'
        context = "\n".join([
            f"â¤ **{item['node_1']}** (Score: {item['score']:.2f}) â†’ {item['relationship']} â†’ **{item['node_2']}**"
            for item in graph_data
        ])

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
        Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î¿Ï‚ ÏˆÎ·Ï†Î¹Î±ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ Î³Î¹Î± Ï„Î¿Î½ **Î•Î¸Î½Î¹ÎºÏŒ ÎšÎ±Ï„Î¬Î»Î¿Î³Î¿ Î¥Ï€Î·ÏÎµÏƒÎ¹ÏÎ½ Ï„Î·Ï‚ Î•Î»Î»Î¬Î´Î±Ï‚**. 
        Î£Î¿Ï… Î´Î¯Î½Ï‰ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ Ï„Î· Î“ÏÎ±Ï†Î¹ÎºÎ® Î’Î¬ÏƒÎ· Î“Î½ÏÏƒÎ·Ï‚ (GraphRAG) ÎºÎ±Î¹ Î¸Î­Î»Ï‰ Î½Î± Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Îµ **Î´Î¿Î¼Î·Î¼Î­Î½Î¿ ÎºÎ±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏÏŒÏ€Î¿ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬**.

        ğŸ”¹ **Î•ÏÏÏ„Î·ÏƒÎ·:** {question}
        ğŸ”¹ **Î£Ï‡ÎµÏ„Î¹ÎºÎ­Ï‚ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚:**
        {context}

        ğŸ”¹ **Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:** Î”ÏÏƒÎµ Î¼Î¹Î± Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ®, ÏƒÎ±Ï†Î® ÎºÎ±Î¹ Î´Î¿Î¼Î·Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬. Î•Î¾Î®Î³Î·ÏƒÎµ Ï„Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±, Ï„Î± Î²Î®Î¼Î±Ï„Î±, ÎºÎ±Î¹ Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚.
        """
    )

    formatted_prompt = prompt.format(context=context, question=user_query)
    response = llm.predict(formatted_prompt)
    return response
